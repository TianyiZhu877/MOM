# âœ¨ğ— ğ—¢ğ— âœ¨: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models 

### ğŸ”Overview

**Memory-efficient Offloaded Mini-sequence Inference (ğ— ğ—¢ğ— )  partitions critical layers into smaller "mini-sequences" and integrates seamlessly with KV cache offloading.**

<img src=".\doc\images\MSIAch.png" width="240"/>   <img src=".\doc\images\memory_speed_tradeoff.png" width="480"/>





### âœ… Features

- ğŸ¦™ **Supports LLaMA 3, Qwen2.5, and Mistral-Nemo**
- ğŸ’¾ **Reduces peak memory usage by over 50% on average**
- ğŸ“ˆ **Extends maximum context length from 155k to 455k tokens on a single A100 80GB GPU**
- ğŸ¯ **Preserves output accuracy â€” generates identical results**
- âš¡ **Minimal computational overhead, with accelerated last-layer processing**

<p align="center">
<img src=".\doc\images\max_context_extended.png" width="400"/>
</p>

<!-- <br/><br/> -->


**The method drastically reduces prefill memory consumption, eliminating it as the longstanding dominant memory bottleneck during inference. This breakthrough fundamentally changes research priorities, redirecting future efforts from prefill-stage optimizations to improving decode-stage residual KV cache efficiency.**

<p align="center">
<img src=".\doc\images\fullCompare.png" width="600"/>
</p>

### ğŸ§ªTesting

```bash
pip install -r requirements.txt 
```

**[Checkout the notebook for ğ— ğ—¢ğ— ]()**

Or run the python scripts in this directory for comparison between different configuration, and visualize the results


